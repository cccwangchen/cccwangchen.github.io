---
layout: post
title: david silver课程笔记七
image: /img/hello_world.jpeg
---

主要介绍一下david大神的[强化学习课程](https://space.bilibili.com/74997410/#/),课程本身质量很不错，但是翻译水平实在很差，极大的加深了理解的难度．　

**第七讲的中心思想是参数化policy,通过一系列梯度算法来优化policy.**  

与基于value的算法相比，policy gradient最大的优势莫过于能够有效的处理连续行动空间的策略优化．至于它所被诟病的效率低，方差大等问题，都可以被后来的加强版policy gradient解决.policy gradient的思想很简单，就是设置一个评价函数，该函数可以评价policy的好坏，根据该函数的梯度一步一步改善policy.这个评价函数为在该policy下回报的期望．也就是说，我们总是朝着回报最大的方向前进．　
实际操作中主要使用Actor-Critic算法，不再采用真实的value function来评估,value function也被参数化了．试想，在使用最朴素的policy gradient时，v(t)的确是Q*　的无偏估计，但v(t)同时是顽固的，我们也许会遇到很接近Ｑ* 价值的v(t),但同时我们也会遇到很小的价值量，往往需要许多轮的迭代到达终点．这正是因为我们不能优化用于评估的价值函数．Actor-Critic解决了它．此外，基线技术，TD-error等等也是基础的小技巧．　　
David 在课上还补充了一些决定性策略梯度的内容，感觉是**DDPG**的简介，但我暂时没有深入的理解．　　

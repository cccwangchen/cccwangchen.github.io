---
layout: post
title: david silver课程笔记七
image: /img/hello_world.jpeg
---

主要介绍一下david大神的[强化学习课程](https://space.bilibili.com/74997410/#/),课程本身质量很不错，但是翻译水平实在很差，极大的加深了理解的难度．　

**第七讲的中心思想是参数化policy,通过一系列梯度算法来优化policy.**  

与基于value的算法相比,policy gradient最大的优势莫过于能够有效的处理连续行动空间的策略优化.至于它所被诟病的效率低,方差大等问题,都可以被加强版policy gradient解决.policy gradient的思想很简单,就是设置一个评价函数,该函数可以评价policy的好坏,根据该函数的梯度一步一步改善policy.这个评价函数为在该策略下回报的期望.也就是说,我们是朝着回报最大的方向前进.

实际操作中主要使用Actor-Critic算法,不再采用真实的价值来评估,即评估价值也被参数化了.试想,在使用最朴素的策略梯度时,抽样的价值的确是Q(s,a)的无偏估计,但它同时是随机不确定的,我们也许会遇到很接近Ｑ(s,a)价值的抽样,但同时我们也会遇到很小很差劲的价值量.往往需要许多轮的迭代获得最优策略.这正是因为我们不能优化用于评估的价值函数.Actor-Critic解决了它.此外,基线技术,使用TD-error等等也是一些优化方差，提高效率的小技巧.　　

David 在课上还补充了一些决定性策略梯度的内容,感觉是[DDPG](https://www.bilibili.com/video/av15990727?from=search&seid=14966112394343272832)的简介,但我暂时没有深入的理解.同时,自然梯度法也在最后被简单提及．
